import streamlit as st
import google.generativeai as genai

# --- 1. FUN√á√ÉO DE CONEX√ÉO (FOR√áA BRUTA) ---
def configurar_ia():
    try:
        # Verifica se a chave existe nos Secrets do Streamlit
        if "GOOGLE_API_KEY" not in st.secrets:
            return None
            
        api_key = st.secrets["GOOGLE_API_KEY"]
        genai.configure(api_key=api_key)
        
        # Lista de for√ßa bruta: tenta do mais moderno para o mais compat√≠vel
        modelos_para_testar = ['gemini-1.5-flash', 'gemini-1.5-pro', 'gemini-pro']
        
        for nome_modelo in modelos_para_testar:
            try:
                model = genai.GenerativeModel(nome_modelo)
                # Teste de "fuma√ßa": gera apenas 1 token para validar a conex√£o
                model.generate_content("oi", generation_config={"max_output_tokens": 1})
                return model # Se funcionar, retorna este modelo imediatamente
            except Exception:
                continue # Se der erro 404 ou 403, pula para o pr√≥ximo da lista
        
        return None
    except Exception:
        return None

# --- 2. INICIALIZA√á√ÉO ---
# O modelo √© carregado uma √∫nica vez ao iniciar o script
model_gemini = configurar_ia()

class LabSmartAI:
    def __init__(self):
        self.model = model_gemini

    def get_ai_answer(self, user_text: str):
        if self.model is None:
            return "Erro: N√£o foi poss√≠vel conectar a nenhum modelo Gemini. Verifique sua chave de API e se a biblioteca google-generativeai est√° no requirements.txt."
        
        try:
            # Comando mestre para garantir resposta em Portugu√™s
            response = self.model.generate_content(
                f"Voc√™ √© um assistente t√©cnico de laborat√≥rio inteligente. Responda em portugu√™s: {user_text}"
            )
            return response.text
        except Exception as e:
            return f"Erro na comunica√ß√£o com a IA: {e}"

# --- 3. INTERFACE DO CHATBOT (A SER CHAMADA PELO APP.PY) ---
def show_chatbot():
    st.header("ü§ñ Assistente Cient√≠fico com IA")

    # Garante que a classe da IA est√° na mem√≥ria da sess√£o
    if "ia_class" not in st.session_state:
        st.session_state.ia_class = LabSmartAI()
    
    bot = st.session_state.ia_class

    # Alerta visual caso a conex√£o falhe completamente
    if bot.model is None:
        st.error("‚ö†Ô∏è Falha na conex√£o de for√ßa bruta. Nenhum modelo (Flash ou Pro) respondeu.")
    else:
        # Mostra qual modelo foi selecionado pelo teste de for√ßa bruta
        st.success(f"Conectado com sucesso ao modelo: {bot.model.model_name}")

    st.divider()

    # Hist√≥rico do Chat
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Campo de entrada (O seu teste do 'Oi')
    if prompt := st.chat_input("Digite 'Oi' para testar a conex√£o..."):
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("IA processando..."):
                resposta = bot.get_ai_answer(prompt)
                st.markdown(resposta)
                st.session_state.chat_history.append({"role": "assistant", "content": resposta})
