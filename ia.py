import streamlit as st
import google.generativeai as genai

# --- 1. CONFIGURA√á√ÉO DE FOR√áA BRUTA (EXTERNA √Ä CLASSE) ---
def inicializar_modelo_seguro():
    """Tenta conectar em todos os modelos poss√≠veis para evitar erro 404."""
    try:
        if "GOOGLE_API_KEY" not in st.secrets:
            return None, "Chave n√£o encontrada nos Secrets"
            
        api_key = st.secrets["GOOGLE_API_KEY"]
        genai.configure(api_key=api_key)
        
        # Lista exaustiva para o teste de for√ßa bruta
        modelos_disponiveis = [
            'gemini-1.5-flash', 
            'gemini-1.5-pro', 
            'gemini-pro', 
            'gemini-1.0-pro',
            'gemini-1.5-flash-8b'
        ]
        
        for nome in modelos_disponiveis:
            try:
                model = genai.GenerativeModel(nome)
                # Teste de comunica√ß√£o (Smoke Test)
                model.generate_content("oi", generation_config={"max_output_tokens": 1})
                return model, nome  # Retorna o primeiro que responder com sucesso
            except Exception:
                continue
                
        return None, "Nenhum modelo dispon√≠vel para esta chave"
    except Exception as e:
        return None, str(e)

# Inicializa√ß√£o global para performance
modelo_global, nome_modelo_ativo = inicializar_modelo_seguro()

# --- 2. CLASSE DE INTEGRA√á√ÉO ---
class LabSmartAI:
    def __init__(self):
        self.model = modelo_global
        self.nome_modelo = nome_modelo_ativo

    def get_ai_answer(self, user_text: str):
        if self.model is None:
            return f"Erro de Conex√£o: {self.nome_modelo}. Verifique sua API Key."
        
        try:
            # Contexto de assistente de laborat√≥rio
            prompt_eng = f"Voc√™ √© um assistente t√©cnico de laborat√≥rio especializado. Responda em portugu√™s: {user_text}"
            response = self.model.generate_content(prompt_eng)
            return response.text
        except Exception as e:
            return f"Erro ao processar consulta com o modelo {self.nome_modelo}: {e}"

# --- 3. FUN√á√ÉO MESTRA (CHAMADA PELO SEU SISTEMA PRINCIPAL) ---
def show_chatbot():
    """Fun√ß√£o de interface chamada pelo app.py"""
    st.header("ü§ñ Assistente Cient√≠fico LabSmart")

    # Garante que a classe est√° instanciada na sess√£o do Streamlit
    if "ia_engine" not in st.session_state:
        st.session_state.ia_engine = LabSmartAI()
    
    bot = st.session_state.ia_engine

    # Painel de Diagn√≥stico (Ajuda a identificar qual modelo funcionou)
    if bot.model:
        st.success(f"‚úÖ Motor de IA Ativo: **{bot.nome_modelo}**")
    else:
        st.error(f"‚ùå Falha Cr√≠tica: {bot.nome_modelo}")
        st.info("Dica: Verifique se o seu requirements.txt cont√©m: google-generativeai>=0.8.3")

    st.divider()

    # Hist√≥rico de Conversas
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Entrada do Usu√°rio para o Teste
    if prompt := st.chat_input("Digite 'Oi' para testar a for√ßa bruta..."):
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner(f"Consultando {bot.nome_modelo}..."):
                resposta = bot.get_ai_answer(prompt)
                st.markdown(resposta)
                st.session_state.chat_history.append({"role": "assistant", "content": resposta})
