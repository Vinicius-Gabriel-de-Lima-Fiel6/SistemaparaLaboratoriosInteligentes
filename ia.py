import streamlit as st
import pandas as pd
from groq import Groq
import cv2
import numpy as np
from ultralytics import YOLO
from datetime import datetime
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase

# --- CONFIGURA√á√ÉO DO MOTOR GROQ ---
def inicializar_motor():
    if "GROQ_API_KEY" not in st.secrets:
        return None, "Configure a GROQ_API_KEY nos Secrets do Streamlit."
    
    # Modelo est√°vel e potente para 2026
    modelo = "llama-3.3-70b-versatile"
    client = Groq(api_key=st.secrets["GROQ_API_KEY"])
    return client, modelo

client_groq, modelo_ativo = inicializar_motor()

# --- PROCESSADOR DE V√çDEO (YOLO VIA NAVEGADOR) ---
class VideoProcessor(VideoTransformerBase):
    def __init__(self, model):
        self.model = model

    def transform(self, frame):
        # Converte o frame recebido do navegador (WebRTC) para array numpy
        img = frame.to_ndarray(format="bgr24")
        
        # Executa a detec√ß√£o do YOLOv8
        results = self.model(img)
        
        # Retorna o frame anotado com as caixas de detec√ß√£o
        return results[0].plot()

# --- CLASSE PRINCIPAL DA IA ---
class LabSmartAI:
    def __init__(self):
        self.yolo_model = None
        self.client = client_groq

    def get_yolo_model(self):
        """Carrega o modelo YOLO apenas se a c√¢mera for ativada"""
        if self.yolo_model is None:
            self.yolo_model = YOLO("yolov8n.pt")
        return self.yolo_model

    def executar_fluxo_agente(self, objetivo, dados=None):
        """L√≥gica de resposta proporcional e cient√≠fica"""
        data_hoje = datetime.now().strftime("%d/%m/%Y")
        
        prompt_sistema = f"""
        Data Atual: {data_hoje}
        Usu√°rio solicitou: {objetivo}
        
        DIRETRIZES:
        1. PROPORCIONALIDADE: Se for uma sauda√ß√£o ou algo simples, responda de forma curta e amig√°vel.
        2. PROFUNDIDADE: Se for um pedido t√©cnico, projeto ou an√°lise, realize uma pesquisa profunda (deep search) em sua base de dados e entregue uma resposta cient√≠fica detalhada.
        3. ESTILO: Resposta limpa, integrada, sem divis√µes de agentes (estilo ChatGPT/Gemini).
        4. DADOS: Se houver dados fornecidos abaixo, incorpore na an√°lise:
        {dados if dados else "Nenhum dado extra fornecido."}
        """

        messages = [
            {"role": "system", "content": "Voc√™ √© o LabSmart AI, um assistente cient√≠fico de alto n√≠vel que adapta o tom √† necessidade do usu√°rio."},
            {"role": "user", "content": prompt_sistema}
        ]

        try:
            res = self.client.chat.completions.create(
                model=modelo_ativo,
                messages=messages,
                temperature=0.4,
                max_tokens=6000
            )
            return res.choices[0].message.content
        except Exception as e:
            return f"Erro na IA: {str(e)}"

# --- INTERFACE STREAMLIT ---
def show_chatbot():
    st.set_page_config(page_title="LabSmart AI", layout="wide")
    st.title("üî¨ LabSmart AI - Hub Cient√≠fico")

    if "ia_engine" not in st.session_state:
        st.session_state.ia_engine = LabSmartAI()
    
    bot = st.session_state.ia_engine

    # --- SIDEBAR ---
    with st.sidebar:
        st.header("üëÅÔ∏è Vis√£o Computacional")
        ativar_camera = st.toggle("Ativar C√¢mera do Notebook")
        
        if ativar_camera:
            st.info("Conectando √† c√¢mera...")
            webrtc_streamer(
                key="yolo-vision",
                video_processor_factory=lambda: VideoProcessor(bot.get_yolo_model()),
                async_processing=True,
                # Servidores STUN robustos para evitar erro de conex√£o
                rtc_configuration={
                    "iceServers": [
                        {"urls": ["stun:stun.l.google.com:19302"]},
                        {"urls": ["stun:stun1.l.google.com:19302"]},
                        {"urls": ["stun:stun.services.mozilla.com"]}
                    ]
                },
                media_stream_constraints={"video": True, "audio": False},
            )
        
        st.divider()
        st.header("üìÇ Arquivos")
        up = st.file_uploader("Upload de CSV ou TXT", type=["csv", "txt"])
        
        if st.button("üóëÔ∏è Limpar Conversa"):
            st.session_state.messages = []
            st.rerun()

    # --- √ÅREA DE CHAT ---
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Exibe hist√≥rico
    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    # Input do usu√°rio
    if prompt := st.chat_input("Como posso ajudar no laborat√≥rio hoje?"):
        # Adiciona mensagem do usu√°rio
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Gera resposta
        with st.chat_message("assistant"):
            with st.spinner("Analisando e pesquisando..."):
                dados_txt = up.getvalue().decode("utf-8", errors="ignore") if up else None
                resposta = bot.executar_fluxo_agente(prompt, dados_txt)
                st.markdown(resposta)
                st.session_state.messages.append({"role": "assistant", "content": resposta})

        # Bot√£o para baixar a resposta
        st.download_button("üì• Baixar Relat√≥rio", resposta, file_name="labsmart_report.md")

if __name__ == "__main__":
    show_chatbot()
